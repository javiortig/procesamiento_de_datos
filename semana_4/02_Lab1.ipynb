{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Lab1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab1 : Spark Word Count\n",
    "\n",
    "### Topics : \n",
    "\n",
    "* RDD Creation\n",
    "* RDD Transformations and Actions\n",
    "\n",
    "### Example objetive :\n",
    "\n",
    "Given an input file , compute the nb of ocurrences of a particular word inside the file\n",
    "\n",
    "### Reference :\n",
    "\n",
    "* SPARK Reference Documentation: https://spark.apache.org/docs/2.3.1/programming-guide.html#rdd-operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import sys\n",
    " print(sys.executable)\n",
    " print(sys.version)\n",
    " print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(__builtins__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers1_99 = sc.parallelize(range(1,100))\n",
    "type(numbers1_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numbers1_99.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile=\"data/appl.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(inputFile)\n",
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of lines \n",
    "%time lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of partitions for this RDD \n",
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the word you want to search for\n",
    "search_word='error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Transformations and actions to compute the result\n",
    "\n",
    "Transformations : \n",
    "    \n",
    "1. flatMap() transformation : split each line into the words that form it , split by whitespace\n",
    "2. filter() transformation:  filter on each line those words that are equal to the search word\n",
    "3. map() transformation : create a tuple with each filtered word on each line and a counter\n",
    "4. reduceByKey() transformation : aggregate based on the keys(=distinct words) with a sum function (add) over all lines\n",
    "\n",
    "Action : \n",
    "    \n",
    "1. collect() : return all elements from the computed RDD\n",
    "\n",
    "Lazy Evaluation :\n",
    "\n",
    "* Until the collect() action is called nothing actually happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_rdd = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : search_word in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)\n",
    "\n",
    "type(counts_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Job Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the RDD lineage\n",
    "print(counts_rdd.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lineage is telling us that there will be 2 stages with 2 tasks each for this spark job\n",
    "# Important points : \n",
    "# 1. see there is a one to one correlation between task and partition\n",
    "# 2. A shuffling of data is involved because the reduceByKey \n",
    "#    requires to place all items belonging to the same key on the same partition \n",
    "#    shuffling operation marks the boundary between stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = counts_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in errors:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Room for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now , imagine we want to search a set of words ...\n",
    "# Do you want to repeat every time the loading and split by whitespace operations ?\n",
    "# These are going to be repeated every time unless we cache ...\n",
    "cached_lines = lines.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now search for other words\n",
    "search_word='info'\n",
    "counts_rdd = cached_lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : search_word in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = counts_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in infos:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis\n",
    "\n",
    "* In the Spark Web UI Inspect the storage tab.\n",
    "* You should see that the RDD has been cached , saved directly in memory\n",
    "* Now perform again and operation , like count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cached_lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now search for other words\n",
    "search_word='notice'\n",
    "counts_rdd = cached_lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : search_word in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notices = counts_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in notices:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
