{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Lab1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Python & Basic Spark Partial Exam\n",
    "\n",
    "__Estos ejercicio no eliminan la necesidad de revisar las diapositivas y los ejercicios hechos en class.__\n",
    "\n",
    "### Reference :\n",
    "\n",
    "* SPARK Reference Documentation: https://spark.apache.org/docs/2.3.1/programming-guide.html#rdd-operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Python:\n",
    "Using sys print Python executable path, Python version and version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n",
      "3.6.9 (default, Oct  8 2020, 12:12:24) \n",
      "[GCC 8.4.0]\n",
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    " import sys\n",
    " print(sys.executable)\n",
    " print(sys.version)\n",
    " print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Python:\n",
    "Print the list of object available in Python and all inside \\_\\_builtins\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Out',\n",
       " '_',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_i3',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'exit',\n",
       " 'findspark',\n",
       " 'get_ipython',\n",
       " 'pyspark',\n",
       " 'quit',\n",
       " 'sc',\n",
       " 'sys']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArithmeticError',\n",
       " 'AssertionError',\n",
       " 'AttributeError',\n",
       " 'BaseException',\n",
       " 'BlockingIOError',\n",
       " 'BrokenPipeError',\n",
       " 'BufferError',\n",
       " 'BytesWarning',\n",
       " 'ChildProcessError',\n",
       " 'ConnectionAbortedError',\n",
       " 'ConnectionError',\n",
       " 'ConnectionRefusedError',\n",
       " 'ConnectionResetError',\n",
       " 'DeprecationWarning',\n",
       " 'EOFError',\n",
       " 'Ellipsis',\n",
       " 'EnvironmentError',\n",
       " 'Exception',\n",
       " 'False',\n",
       " 'FileExistsError',\n",
       " 'FileNotFoundError',\n",
       " 'FloatingPointError',\n",
       " 'FutureWarning',\n",
       " 'GeneratorExit',\n",
       " 'IOError',\n",
       " 'ImportError',\n",
       " 'ImportWarning',\n",
       " 'IndentationError',\n",
       " 'IndexError',\n",
       " 'InterruptedError',\n",
       " 'IsADirectoryError',\n",
       " 'KeyError',\n",
       " 'KeyboardInterrupt',\n",
       " 'LookupError',\n",
       " 'MemoryError',\n",
       " 'ModuleNotFoundError',\n",
       " 'NameError',\n",
       " 'None',\n",
       " 'NotADirectoryError',\n",
       " 'NotImplemented',\n",
       " 'NotImplementedError',\n",
       " 'OSError',\n",
       " 'OverflowError',\n",
       " 'PendingDeprecationWarning',\n",
       " 'PermissionError',\n",
       " 'ProcessLookupError',\n",
       " 'RecursionError',\n",
       " 'ReferenceError',\n",
       " 'ResourceWarning',\n",
       " 'RuntimeError',\n",
       " 'RuntimeWarning',\n",
       " 'StopAsyncIteration',\n",
       " 'StopIteration',\n",
       " 'SyntaxError',\n",
       " 'SyntaxWarning',\n",
       " 'SystemError',\n",
       " 'SystemExit',\n",
       " 'TabError',\n",
       " 'TimeoutError',\n",
       " 'True',\n",
       " 'TypeError',\n",
       " 'UnboundLocalError',\n",
       " 'UnicodeDecodeError',\n",
       " 'UnicodeEncodeError',\n",
       " 'UnicodeError',\n",
       " 'UnicodeTranslateError',\n",
       " 'UnicodeWarning',\n",
       " 'UserWarning',\n",
       " 'ValueError',\n",
       " 'Warning',\n",
       " 'ZeroDivisionError',\n",
       " '__IPYTHON__',\n",
       " '__build_class__',\n",
       " '__debug__',\n",
       " '__doc__',\n",
       " '__import__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'abs',\n",
       " 'all',\n",
       " 'any',\n",
       " 'ascii',\n",
       " 'bin',\n",
       " 'bool',\n",
       " 'bytearray',\n",
       " 'bytes',\n",
       " 'callable',\n",
       " 'chr',\n",
       " 'classmethod',\n",
       " 'compile',\n",
       " 'complex',\n",
       " 'copyright',\n",
       " 'credits',\n",
       " 'delattr',\n",
       " 'dict',\n",
       " 'dir',\n",
       " 'display',\n",
       " 'divmod',\n",
       " 'enumerate',\n",
       " 'eval',\n",
       " 'exec',\n",
       " 'filter',\n",
       " 'float',\n",
       " 'format',\n",
       " 'frozenset',\n",
       " 'get_ipython',\n",
       " 'getattr',\n",
       " 'globals',\n",
       " 'hasattr',\n",
       " 'hash',\n",
       " 'help',\n",
       " 'hex',\n",
       " 'id',\n",
       " 'input',\n",
       " 'int',\n",
       " 'isinstance',\n",
       " 'issubclass',\n",
       " 'iter',\n",
       " 'len',\n",
       " 'license',\n",
       " 'list',\n",
       " 'locals',\n",
       " 'map',\n",
       " 'max',\n",
       " 'memoryview',\n",
       " 'min',\n",
       " 'next',\n",
       " 'object',\n",
       " 'oct',\n",
       " 'open',\n",
       " 'ord',\n",
       " 'pow',\n",
       " 'print',\n",
       " 'property',\n",
       " 'range',\n",
       " 'repr',\n",
       " 'reversed',\n",
       " 'round',\n",
       " 'set',\n",
       " 'setattr',\n",
       " 'slice',\n",
       " 'sorted',\n",
       " 'staticmethod',\n",
       " 'str',\n",
       " 'sum',\n",
       " 'super',\n",
       " 'tuple',\n",
       " 'type',\n",
       " 'vars',\n",
       " 'zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(__builtins__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Spark:\n",
    "Print the spark context\n",
    "\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "SparkContext\n",
    "\n",
    "Spark UI\n",
    "\n",
    "Version\n",
    "\n",
    "    v3.0.1\n",
    "\n",
    "Master\n",
    "\n",
    "    local[*]\n",
    "\n",
    "AppName\n",
    "\n",
    "    Lab1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.209.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Lab1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Python:\n",
    "Create a range of numbers between 1 and 99 in Python and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(range(1,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - PySpark:\n",
    "Create a range of numbers between 1 and 99 in Spark and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "pyspark.rdd.PipelinedRDD\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers1_99 = sc.parallelize(range(1,100))\n",
    "type(numbers1_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6- PySpark:\n",
    "Create a range of numbers between 1 and 99 in Spark and using a spark action obtain the sum of all numbers.\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "4950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n"
     ]
    }
   ],
   "source": [
    "print(numbers1_99.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7- PySpark:\n",
    "Read the file app.log into a Python variable and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "pyspark.rdd.RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile=\"app.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(inputFile)\n",
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8- PySpark & Python:\n",
    "Read the file app.log, count all lines and usint the Python method timeit calculate the time it takes to do the operation\n",
    "\n",
    "__Expected result (aproximately):__\n",
    "\n",
    "CPU times: user 0 ns, sys: 15.6 ms, total: 15.6 ms\n",
    "\n",
    "Wall time: 95.5 ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 15.6 ms, total: 15.6 ms\n",
      "Wall time: 238 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of lines \n",
    "%time sc.textFile(\"app.log\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 - PySpark:\n",
    "using sc.textFile(\"app.log\") print the number of partitions for this RDD Spark will use when an action is performed\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of partitions for this RDD \n",
    "sc.textFile(\"app.log\").getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - PySpark:\n",
    "using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"app.log\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Transformations and actions to compute the result\n",
    "\n",
    "Transformations : \n",
    "    \n",
    "1. flatMap() transformation : split each line into the words that form it , split by whitespace\n",
    "2. filter() transformation:  filter on each line those words that are equal to the search word \"error\"\n",
    "3. map() transformation : create a tuple with each filtered word on each line and a counter\n",
    "4. reduceByKey() transformation : aggregate based on the keys(=distinct words) with a sum function (add) over all lines\n",
    "\n",
    "Action : \n",
    "    \n",
    "1. collect() : return all elements from the computed RDD\n",
    "\n",
    "Lazy Evaluation :\n",
    "\n",
    "* Until the collect() action is called nothing actually happens\n",
    "\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the word you want to search for\n",
    "search_word='error'\n",
    "from operator import add\n",
    "counts_rdd = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : search_word in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)\n",
    "\n",
    "type(counts_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[error]', 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_rdd .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - PySpark:\n",
    "Using the command create for previous exercise (only transformation without the collect action) print the debug string using .toDebugString().decode(\"utf-8\")\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "\n",
    "(2) PythonRDD[12] at RDD at PythonRDD.scala:53 []\n",
    "\n",
    "|  MapPartitionsRDD[11] at mapPartitions at PythonRDD.scala:133 []\n",
    "\n",
    "|  ShuffledRDD[10] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "\n",
    "+-(2) PairwiseRDD[9] at reduceByKey at <ipython-input-19-e4397b132399>:4 []\n",
    "\n",
    "    |  PythonRDD[8] at reduceByKey at <ipython-input-19-e4397b132399>:4 []\n",
    "    \n",
    "    |  app.log MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "    \n",
    "    |  app.log HadoopRDD[5] at textFile at NativeMethodAccessorImpl.java:0 []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[15] at collect at <ipython-input-16-0b0ba81e427c>:1 []\n",
      " |  MapPartitionsRDD[14] at mapPartitions at PythonRDD.scala:133 []\n",
      " |  ShuffledRDD[13] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[12] at reduceByKey at <ipython-input-15-34e96636d3eb>:7 []\n",
      "    |  PythonRDD[11] at reduceByKey at <ipython-input-15-34e96636d3eb>:7 []\n",
      "    |  app.log MapPartitionsRDD[10] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  app.log HadoopRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "# See the RDD lineage\n",
    "print(counts_rdd.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lineage is telling us that there will be 2 stages with 2 tasks each for this spark job\n",
    "# Important points : \n",
    "# 1. see there is a one to one correlation between task and partition\n",
    "# 2. A shuffling of data is involved because the reduceByKey \n",
    "#    requires to place all items belonging to the same key on the same partition \n",
    "#    shuffling operation marks the boundary between stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = counts_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[error]: 5\n"
     ]
    }
   ],
   "source": [
    "for word, count in errors:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 - PySpark:\n",
    "\n",
    "Now implement a Python loop changing the filter word to the following list ['error','info','notice'] (does not pass this list into the spark filter), filter only one element. I want it to be this bad!\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]\n",
    "\n",
    "[('[info]', 96)]\n",
    "\n",
    "[('[notice]', 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[error]', 5)]\n",
      "[('[info]', 96)]\n",
      "[('[notice]', 2)]\n"
     ]
    }
   ],
   "source": [
    "for search_word in ['error','info','notice']:\n",
    "    counts_rdd = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "            .filter(lambda x : search_word in x) \\\n",
    "            .map(lambda word : (word, 1)) \\\n",
    "            .reduceByKey(add).collect()\n",
    "    print(counts_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12 - PySpark:\n",
    "\n",
    "Now implement a Python loop changing the filter word to the following list ['error','info','notice'] (does not pass this list into the spark filter), filter only one element. \n",
    "Now think about optimization, implement the same code using persit or cache() in the correct place.\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]\n",
    "\n",
    "[('[info]', 96)]\n",
    "\n",
    "[('[notice]', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[error]', 5)]\n",
      "[('[info]', 96)]\n",
      "[('[notice]', 2)]\n"
     ]
    }
   ],
   "source": [
    "split_lines_cached = lines.flatMap(lambda x: x.split(' ')).cache()\n",
    "for search_word in ['error','info','notice']:\n",
    "    counts_rdd = split_lines_cached.filter(lambda x : search_word in x) \\\n",
    "    .map(lambda word : (word, 1)) \\\n",
    "    .reduceByKey(add).collect()\n",
    "    print(counts_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13 - PySpark:\n",
    "\n",
    "Now pass the list ['error','info','notice'] into the spark filter and using __Python list comprehension__ filter all lines counts the number of ocurrences of lines with 'error', 'info' and 'notice', one unique spark call allowed.\n",
    "\n",
    "__Expected result:__ \n",
    "\n",
    "[('[notice]', 2), ('[info]', 96), ('[error]', 5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[notice]', 2), ('[info]', 96), ('[error]', 5)]\n"
     ]
    }
   ],
   "source": [
    "search_word_list =  ['error','info','notice'] \n",
    "counts_rdd = split_lines_cached.filter(lambda x : sum([i in x for i in search_word_list])) \\\n",
    ".map(lambda word : (word, 1)) \\\n",
    ".reduceByKey(add).collect()\n",
    "print(counts_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
