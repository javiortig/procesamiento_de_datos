{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 20:17:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/13 20:17:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Lab1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Python & Basic Spark Partial Exam\n",
    "\n",
    "__Estos ejercicio no eliminan la necesidad de revisar las diapositivas y los ejercicios hechos en class.__\n",
    "\n",
    "\n",
    "### Reference :\n",
    "\n",
    "* SPARK Reference Documentation: https://spark.apache.org/docs/2.3.1/programming-guide.html#rdd-operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Python:\n",
    "Using sys print Python executable path, Python version and version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  /home/javiortig/.conda/envs/procesamiento_datos/bin/python\n",
      "Version:  3.9.16 (main, Jan 11 2023, 16:05:54) \n",
      "[GCC 11.2.0]\n",
      "Version info:  sys.version_info(major=3, minor=9, micro=16, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Path: \", sys.executable)\n",
    "print(\"Version: \", sys.version)\n",
    "print(\"Version info: \", sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Python:\n",
    "Print the list of object available in Python and all inside \\_\\_builtins\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    " import sys\n",
    " print(sys.executable)\n",
    " print(sys.version)\n",
    " print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__session__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'exit', 'findspark', 'get_ipython', 'open', 'pyspark', 'quit', 'sc']\n"
     ]
    }
   ],
   "source": [
    "print(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.id=local-1678736520349\n",
      "spark.app.name=Lab1\n",
      "spark.app.startTime=1678736519506\n",
      "spark.app.submitTime=1678736519329\n",
      "spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.driver.host=fedora\n",
      "spark.driver.port=44471\n",
      "spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.executor.id=driver\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=\n",
      "spark.ui.showConsoleProgress=true\n"
     ]
    }
   ],
   "source": [
    "#print(sc.getConf().toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Spark:\n",
    "Print the spark context\n",
    "\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "SparkContext\n",
    "\n",
    "Spark UI\n",
    "\n",
    "Version\n",
    "\n",
    "    v3.0.1\n",
    "\n",
    "Master\n",
    "\n",
    "    local[*]\n",
    "\n",
    "AppName\n",
    "\n",
    "    Lab1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Python:\n",
    "Create a range of numbers between 1 and 99 in Python and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'range'>\n"
     ]
    }
   ],
   "source": [
    "print(str(type(range(1, 100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - PySpark:\n",
    "Create a range of numbers between 1 and 99 in Spark and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "pyspark.rdd.PipelinedRDD\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "b = sc.parallelize(range(1,100))\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6- PySpark:\n",
    "Create a range of numbers between 1 and 99 in Spark and using a spark action obtain the sum of all numbers.\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "4950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1, 100)).reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7- PySpark:\n",
    "Read the file app.log into a Python variable and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "pyspark.rdd.RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lines = sc.textFile(\"./app.log\")\n",
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8- PySpark & Python:\n",
    "Read the file app.log, count all lines and usint the Python method timeit calculate the time it takes to do the operation\n",
    "\n",
    "__Expected result (aproximately):__\n",
    "\n",
    "CPU times: user 0 ns, sys: 15.6 ms, total: 15.6 ms\n",
    "\n",
    "Wall time: 95.5 ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.69 ms, sys: 1.72 ms, total: 4.4 ms\n",
      "Wall time: 77.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"./app.log\")\n",
    "%time lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 - PySpark:\n",
    "using sc.textFile(\"app.log\") print the number of partitions for this RDD Spark will use when an action is performed\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - PySpark:\n",
    "using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[error]', 5)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Transformations and actions to compute the result\n",
    "\n",
    "Transformations : \n",
    "    \n",
    "1. flatMap() transformation : split each line into the words that form it , split by whitespace\n",
    "2. filter() transformation:  filter on each line those words that are equal to the search word \"error\"\n",
    "3. map() transformation : create a tuple with each filtered word on each line and a counter\n",
    "4. reduceByKey() transformation : aggregate based on the keys(=distinct words) with a sum function (add) over all lines\n",
    "\n",
    "Action : \n",
    "    \n",
    "1. collect() : return all elements from the computed RDD\n",
    "\n",
    "Lazy Evaluation :\n",
    "\n",
    "* Until the collect() action is called nothing actually happens\n",
    "\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(lambda x: x.split(' ')) \\\n",
    "    .filter(lambda x: 'error' in x) \\\n",
    "    .map(lambda word : (word, 1)) \\\n",
    "    .reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - PySpark:\n",
    "Using the command create for previous exercise (only transformation without the collect action) print the debug string using .toDebugString().decode(\"utf-8\")\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "\n",
    "(2) PythonRDD[12] at RDD at PythonRDD.scala:53 []\n",
    "\n",
    "|  MapPartitionsRDD[11] at mapPartitions at PythonRDD.scala:133 []\n",
    "\n",
    "|  ShuffledRDD[10] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "\n",
    "+-(2) PairwiseRDD[9] at reduceByKey at <ipython-input-19-e4397b132399>:4 []\n",
    "\n",
    "    |  PythonRDD[8] at reduceByKey at <ipython-input-19-e4397b132399>:4 []\n",
    "    \n",
    "    |  app.log MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "    \n",
    "    |  app.log HadoopRDD[5] at textFile at NativeMethodAccessorImpl.java:0 []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 - PySpark:\n",
    "\n",
    "Now implement a Python loop changing the filter word to the following list ['error','info','notice'] (does not pass this list into the spark filter), filter only one element. I want it to be this bad!\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]\n",
    "\n",
    "[('[info]', 96)]\n",
    "\n",
    "[('[notice]', 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12 - PySpark:\n",
    "\n",
    "Now implement a Python loop changing the filter word to the following list ['error','info','notice'] (does not pass this list into the spark filter), filter only one element. \n",
    "Now think about optimization, implement the same code using persit or cache() in the correct place.\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]\n",
    "\n",
    "[('[info]', 96)]\n",
    "\n",
    "[('[notice]', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13 - PySpark:\n",
    "\n",
    "Now pass the list ['error','info','notice'] into the spark filter and using __Python list comprehension__ filter all lines counts the number of ocurrences of lines with 'error', 'info' and 'notice', one unique spark call allowed.\n",
    "\n",
    "__Expected result:__ \n",
    "\n",
    "[('[notice]', 2), ('[info]', 96), ('[error]', 5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
