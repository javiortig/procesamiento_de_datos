{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/16 09:27:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Lab1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Python & Basic Spark Partial Exam\n",
    "\n",
    "__Estos ejercicio no eliminan la necesidad de revisar las diapositivas y los ejercicios hechos en class.__\n",
    "\n",
    "### Reference :\n",
    "\n",
    "* SPARK Reference Documentation: https://spark.apache.org/docs/2.3.1/programming-guide.html#rdd-operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Python:\n",
    "Using sys print Python executable path, Python version and version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  /home/javiortig/.conda/envs/procesamiento_datos/bin/python\n",
      "Version:  3.9.16 (main, Jan 11 2023, 16:05:54) \n",
      "[GCC 11.2.0]\n",
      "Version info:  sys.version_info(major=3, minor=9, micro=16, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Path: \", sys.executable)\n",
    "print(\"Version: \", sys.version)\n",
    "print(\"Version info: \", sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Python:\n",
    "Print the list of object available in Python and all inside \\_\\_builtins\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Out',\n",
       " '_',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_i3',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'exit',\n",
       " 'findspark',\n",
       " 'get_ipython',\n",
       " 'pyspark',\n",
       " 'quit',\n",
       " 'sc',\n",
       " 'sys']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArithmeticError',\n",
       " 'AssertionError',\n",
       " 'AttributeError',\n",
       " 'BaseException',\n",
       " 'BlockingIOError',\n",
       " 'BrokenPipeError',\n",
       " 'BufferError',\n",
       " 'BytesWarning',\n",
       " 'ChildProcessError',\n",
       " 'ConnectionAbortedError',\n",
       " 'ConnectionError',\n",
       " 'ConnectionRefusedError',\n",
       " 'ConnectionResetError',\n",
       " 'DeprecationWarning',\n",
       " 'EOFError',\n",
       " 'Ellipsis',\n",
       " 'EnvironmentError',\n",
       " 'Exception',\n",
       " 'False',\n",
       " 'FileExistsError',\n",
       " 'FileNotFoundError',\n",
       " 'FloatingPointError',\n",
       " 'FutureWarning',\n",
       " 'GeneratorExit',\n",
       " 'IOError',\n",
       " 'ImportError',\n",
       " 'ImportWarning',\n",
       " 'IndentationError',\n",
       " 'IndexError',\n",
       " 'InterruptedError',\n",
       " 'IsADirectoryError',\n",
       " 'KeyError',\n",
       " 'KeyboardInterrupt',\n",
       " 'LookupError',\n",
       " 'MemoryError',\n",
       " 'ModuleNotFoundError',\n",
       " 'NameError',\n",
       " 'None',\n",
       " 'NotADirectoryError',\n",
       " 'NotImplemented',\n",
       " 'NotImplementedError',\n",
       " 'OSError',\n",
       " 'OverflowError',\n",
       " 'PendingDeprecationWarning',\n",
       " 'PermissionError',\n",
       " 'ProcessLookupError',\n",
       " 'RecursionError',\n",
       " 'ReferenceError',\n",
       " 'ResourceWarning',\n",
       " 'RuntimeError',\n",
       " 'RuntimeWarning',\n",
       " 'StopAsyncIteration',\n",
       " 'StopIteration',\n",
       " 'SyntaxError',\n",
       " 'SyntaxWarning',\n",
       " 'SystemError',\n",
       " 'SystemExit',\n",
       " 'TabError',\n",
       " 'TimeoutError',\n",
       " 'True',\n",
       " 'TypeError',\n",
       " 'UnboundLocalError',\n",
       " 'UnicodeDecodeError',\n",
       " 'UnicodeEncodeError',\n",
       " 'UnicodeError',\n",
       " 'UnicodeTranslateError',\n",
       " 'UnicodeWarning',\n",
       " 'UserWarning',\n",
       " 'ValueError',\n",
       " 'Warning',\n",
       " 'ZeroDivisionError',\n",
       " '__IPYTHON__',\n",
       " '__build_class__',\n",
       " '__debug__',\n",
       " '__doc__',\n",
       " '__import__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'abs',\n",
       " 'all',\n",
       " 'any',\n",
       " 'ascii',\n",
       " 'bin',\n",
       " 'bool',\n",
       " 'breakpoint',\n",
       " 'bytearray',\n",
       " 'bytes',\n",
       " 'callable',\n",
       " 'chr',\n",
       " 'classmethod',\n",
       " 'compile',\n",
       " 'complex',\n",
       " 'copyright',\n",
       " 'credits',\n",
       " 'delattr',\n",
       " 'dict',\n",
       " 'dir',\n",
       " 'display',\n",
       " 'divmod',\n",
       " 'enumerate',\n",
       " 'eval',\n",
       " 'exec',\n",
       " 'execfile',\n",
       " 'filter',\n",
       " 'float',\n",
       " 'format',\n",
       " 'frozenset',\n",
       " 'get_ipython',\n",
       " 'getattr',\n",
       " 'globals',\n",
       " 'hasattr',\n",
       " 'hash',\n",
       " 'help',\n",
       " 'hex',\n",
       " 'id',\n",
       " 'input',\n",
       " 'int',\n",
       " 'isinstance',\n",
       " 'issubclass',\n",
       " 'iter',\n",
       " 'len',\n",
       " 'license',\n",
       " 'list',\n",
       " 'locals',\n",
       " 'map',\n",
       " 'max',\n",
       " 'memoryview',\n",
       " 'min',\n",
       " 'next',\n",
       " 'object',\n",
       " 'oct',\n",
       " 'open',\n",
       " 'ord',\n",
       " 'pow',\n",
       " 'print',\n",
       " 'property',\n",
       " 'range',\n",
       " 'repr',\n",
       " 'reversed',\n",
       " 'round',\n",
       " 'runfile',\n",
       " 'set',\n",
       " 'setattr',\n",
       " 'slice',\n",
       " 'sorted',\n",
       " 'staticmethod',\n",
       " 'str',\n",
       " 'sum',\n",
       " 'super',\n",
       " 'tuple',\n",
       " 'type',\n",
       " 'vars',\n",
       " 'zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(__builtins__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Spark:\n",
    "Print the spark context\n",
    "\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "SparkContext\n",
    "\n",
    "Spark UI\n",
    "\n",
    "Version\n",
    "\n",
    "    v3.0.1\n",
    "\n",
    "Master\n",
    "\n",
    "    local[*]\n",
    "\n",
    "AppName\n",
    "\n",
    "    Lab1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fedora:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Lab1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Python:\n",
    "Create a range of numbers between 1 and 99 in Python and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(range(1,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - PySpark:\n",
    "Create a range of numbers between 1 and 99 in Spark and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "pyspark.rdd.PipelinedRDD\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers1_99 = sc.parallelize(range(1,100))\n",
    "type(numbers1_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6- PySpark:\n",
    "Create a range of numbers between 1 and 99 in Spark and using a spark action obtain the sum of all numbers.\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "4950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                        (0 + 12) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(numbers1_99.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7- PySpark:\n",
    "Read the file app.log into a Python variable and print its type\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "pyspark.rdd.RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile=\"app.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(inputFile)\n",
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8- PySpark & Python:\n",
    "Read the file app.log, count all lines and usint the Python method timeit calculate the time it takes to do the operation\n",
    "\n",
    "__Expected result (aproximately):__\n",
    "\n",
    "CPU times: user 0 ns, sys: 15.6 ms, total: 15.6 ms\n",
    "\n",
    "Wall time: 95.5 ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.48 ms, sys: 1.52 ms, total: 8 ms\n",
      "Wall time: 218 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of lines \n",
    "%time sc.textFile(\"app.log\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 - PySpark:\n",
    "using sc.textFile(\"app.log\") print the number of partitions for this RDD Spark will use when an action is performed\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of partitions for this RDD \n",
    "sc.textFile(\"app.log\").getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - PySpark:\n",
    "using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"app.log\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Transformations and actions to compute the result\n",
    "\n",
    "Transformations : \n",
    "    \n",
    "1. flatMap() transformation : split each line into the words that form it , split by whitespace\n",
    "2. filter() transformation:  filter on each line those words that are equal to the search word \"error\"\n",
    "3. map() transformation : create a tuple with each filtered word on each line and a counter\n",
    "4. reduceByKey() transformation : aggregate based on the keys(=distinct words) with a sum function (add) over all lines\n",
    "\n",
    "Action : \n",
    "    \n",
    "1. collect() : return all elements from the computed RDD\n",
    "\n",
    "Lazy Evaluation :\n",
    "\n",
    "* Until the collect() action is called nothing actually happens\n",
    "\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the word you want to search for\n",
    "search_word='error'\n",
    "from operator import add\n",
    "counts_rdd = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : search_word in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add) # Se puede usar una lambda x, y: x+y en su lugar\n",
    "\n",
    "type(counts_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[error]', 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_rdd .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - PySpark:\n",
    "Using the command create for previous exercise (only transformation without the collect action) print the debug string using .toDebugString().decode(\"utf-8\")\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "\n",
    "(2) PythonRDD[12] at RDD at PythonRDD.scala:53 []\n",
    "\n",
    "|  MapPartitionsRDD[11] at mapPartitions at PythonRDD.scala:133 []\n",
    "\n",
    "|  ShuffledRDD[10] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "\n",
    "+-(2) PairwiseRDD[9] at reduceByKey at <ipython-input-19-e4397b132399>:4 []\n",
    "\n",
    "    |  PythonRDD[8] at reduceByKey at <ipython-input-19-e4397b132399>:4 []\n",
    "    \n",
    "    |  app.log MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "    \n",
    "    |  app.log HadoopRDD[5] at textFile at NativeMethodAccessorImpl.java:0 []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[15] at collect at /tmp/ipykernel_4204/2545033379.py:1 []\n",
      " |  MapPartitionsRDD[14] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[13] at partitionBy at DirectMethodHandleAccessor.java:104 []\n",
      " +-(2) PairwiseRDD[12] at reduceByKey at /tmp/ipykernel_4204/223590160.py:4 []\n",
      "    |  PythonRDD[11] at reduceByKey at /tmp/ipykernel_4204/223590160.py:4 []\n",
      "    |  app.log MapPartitionsRDD[10] at textFile at DirectMethodHandleAccessor.java:104 []\n",
      "    |  app.log HadoopRDD[9] at textFile at DirectMethodHandleAccessor.java:104 []\n"
     ]
    }
   ],
   "source": [
    "# See the RDD lineage\n",
    "print(counts_rdd.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lineage is telling us that there will be 2 stages with 2 tasks each for this spark job\n",
    "# Important points : \n",
    "# 1. see there is a one to one correlation between task and partition\n",
    "# 2. A shuffling of data is involved because the reduceByKey \n",
    "#    requires to place all items belonging to the same key on the same partition \n",
    "#    shuffling operation marks the boundary between stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = counts_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[error]: 5\n"
     ]
    }
   ],
   "source": [
    "for word, count in errors:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 - PySpark:\n",
    "\n",
    "Now implement a Python loop changing the filter word to the following list ['error','info','notice'] (does not pass this list into the spark filter), filter only one element. I want it to be this bad!\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]\n",
    "\n",
    "[('[info]', 96)]\n",
    "\n",
    "[('[notice]', 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[error]', 5)]\n",
      "[('[info]', 96)]\n",
      "[('[notice]', 2)]\n"
     ]
    }
   ],
   "source": [
    "for search_word in ['error','info','notice']:\n",
    "    counts_rdd = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "            .filter(lambda x : search_word in x) \\\n",
    "            .map(lambda word : (word, 1)) \\\n",
    "            .reduceByKey(add).collect()\n",
    "    print(counts_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12 - PySpark:\n",
    "\n",
    "Now implement a Python loop changing the filter word to the following list ['error','info','notice'] (does not pass this list into the spark filter), filter only one element. \n",
    "Now think about optimization, implement the same code using persit or cache() in the correct place.\n",
    "\n",
    "__Expected result:__\n",
    "\n",
    "[('[error]', 5)]\n",
    "\n",
    "[('[info]', 96)]\n",
    "\n",
    "[('[notice]', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[error]', 5)]\n",
      "[('[info]', 96)]\n",
      "[('[notice]', 2)]\n"
     ]
    }
   ],
   "source": [
    "split_lines_cached = lines.flatMap(lambda x: x.split(' ')).cache()\n",
    "for search_word in ['error','info','notice']:\n",
    "    counts_rdd = split_lines_cached.filter(lambda x : search_word in x) \\\n",
    "    .map(lambda word : (word, 1)) \\\n",
    "    .reduceByKey(add).collect()\n",
    "    print(counts_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13 - PySpark:\n",
    "\n",
    "Now pass the list ['error','info','notice'] into the spark filter and using __Python list comprehension__ filter all lines counts the number of ocurrences of lines with 'error', 'info' and 'notice', one unique spark call allowed.\n",
    "\n",
    "__Expected result:__ \n",
    "\n",
    "[('[notice]', 2), ('[info]', 96), ('[error]', 5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[notice]', 2), ('[info]', 96), ('[error]', 5)]\n"
     ]
    }
   ],
   "source": [
    "search_word_list =  ['error','info','notice'] \n",
    "counts_rdd = split_lines_cached.filter(lambda x : sum([i in x for i in search_word_list])) \\\n",
    ".map(lambda word : (word, 1)) \\\n",
    ".reduceByKey(add).collect()\n",
    "print(counts_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 1, 3: 2})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ej 5\n",
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ej 6\n",
    "rdd1 = sc.parallelize([1,2,3])\n",
    "\n",
    "rdd2 = sc.parallelize([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:===================================>                  (95 + 13) / 144]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['(1+4)=5',\n",
       " '(1+5)=6',\n",
       " '(1+6)=7',\n",
       " '(2+4)=6',\n",
       " '(2+5)=7',\n",
       " '(2+6)=8',\n",
       " '(3+4)=7',\n",
       " '(3+5)=8',\n",
       " '(3+6)=9']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.cartesian(rdd2).map(lambda x: \"({}+{})={}\".format(x[0],x[1],x[0]+x[1])).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers1_99 = sc.parallelize(range(1,100))\n",
    "\n",
    "type(numbers1_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ej 11\n",
    "type(sc.textFile(\"app.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ej 12\n",
    "sc.parallelize(range(1000)).map(lambda x: 2 * x).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'linea': 3, '1': 1, 'Python': 2, '2': 1, '3': 1, 'Spark': 1})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ej 14\n",
    "rdd=sc.parallelize([\"linea 1 Python\",\"linea 2 Python\",\"linea 3 Spark\"] )\n",
    "\n",
    "rdd.flatMap(lambda x: x.split()).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166666833333"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ej 15\n",
    "\n",
    "sc.parallelize(range(1,1000001)).filter(lambda x: x%3==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 6)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ej 16\n",
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "\n",
    "\n",
    "\n",
    "rdd.groupByKey().map(lambda x : (x[0], max(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'a', True, 40]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,\"a\",True]\n",
    "\n",
    "b = a \n",
    "\n",
    "b.append(40)\n",
    "\n",
    "b = a.copy() \n",
    "\n",
    "b.append(40)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [(1,2),(3,4)]\n",
    "\n",
    "b = sc.parallelize(a)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 166666833333)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ej 26\n",
    "rdd = sc.parallelize(range(1,1000001))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rdd.map(lambda x: (x%3, x)).reduceByKey(lambda x,y: x + y).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "5 \n",
      "6 \n",
      "7 \n",
      "8 \n",
      "9 \n",
      "10 \n"
     ]
    }
   ],
   "source": [
    "# Ej 27\n",
    "import sys\n",
    "import math\n",
    " \n",
    "def func1(nums):\n",
    "    \"\"\"func1\"\"\"\n",
    "    stats = nums.stats()\n",
    "    stddev = math.sqrt(stats.variance())\n",
    "    return nums.filter(lambda x: math.fabs(x - stats.mean()) < 3 * stddev)\n",
    " \n",
    "nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1000])\n",
    "output = sorted(func1(nums).collect())\n",
    "for num in output:\n",
    "    print (\"{} \".format(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 29\n",
    "sc.parallelize(range(1000)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333334000000, 166666500000)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#30 \n",
    "def dos_o_no(x):\n",
    "    if x%3==2:\n",
    "        return(2)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "ceroyuno = 0\n",
    "\n",
    "dos = 0\n",
    "\n",
    "for i in range(1,1000001):\n",
    "    if(i %3 == 2):\n",
    "        dos += i\n",
    "    else:\n",
    "        ceroyuno += i\n",
    "        \n",
    "(ceroyuno, dos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 166666833333), (2, 333333666667)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd. \\\n",
    "map(lambda x: (2 if x%3 else 0, x)).reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 5]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 31\n",
    "rdd1 = sc.parallelize([3,4,5,2,5,1,4,6])\n",
    "rdd1.distinct().top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 1), ('Python', 1)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ej 32\n",
    "lines = sc.parallelize([\"linea 1 Python\",\"linea 2 Python\",\"linea 3 Spark\"])\n",
    "from operator import add\n",
    "\n",
    "lines.flatMap(lambda x: x.split(' ')). \\\n",
    "filter(lambda x : \"Python\" in x).map(lambda word : (word, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.defaultdict' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4204/3788909563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pandas\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i like pandas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountByValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"{} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.defaultdict' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "# 33\n",
    "lines = sc.parallelize([\"pandas\", \"i like pandas\"])\n",
    "result = lines.flatMap(lambda x: x.split(\" \")).countByValue()\n",
    "for key, value in result.iteritems():\n",
    "    print (\"{} {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x7fbf285911f0>\n"
     ]
    }
   ],
   "source": [
    "# 34\n",
    "\n",
    "a = filter(lambda x : \"Python\" in x, [\"Python List\"])\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [2]), (3, [4, 6])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#36\n",
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#38\n",
    "sc.parallelize([(1, 2), (3, 4), (3, 6)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 39\n",
    "sc.parallelize(range(1,100)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1406"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 37\n",
    "lines = sc.textFile (\"apache.access.log.txt\").count() \n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7\n",
    "file = sc.textFile(\"apache.access.log.txt\")\n",
    "file.map(lambda x: x.split(' ')).filter(lambda x: x[5] == '\\\"POST').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile (\"app.log\").getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[Sun', '[notice]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[notice]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', 'statistics:'),\n",
       " ('[Sun', 'statistics:'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[error]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Sun', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[error]'),\n",
       " ('[Mon', '[error]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]'),\n",
       " ('[Mon', '[info]')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = sc.textFile(\"app.log\")\n",
    "\n",
    "log.map(lambda x: x.split()).map(lambda x: (x[0], x[5])).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:05:49',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables',\n",
       "  'HTTP/1.1\"',\n",
       "  '401',\n",
       "  '12846'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:06:51',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/rdiff/TWiki/NewUserTemplate?rev1=1.3&rev2=1.2',\n",
       "  'HTTP/1.1\"',\n",
       "  '200',\n",
       "  '4523'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:10:02',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/mailman/listinfo/hsdivision',\n",
       "  'HTTP/1.1\"',\n",
       "  '200',\n",
       "  '6291'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:11:58',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/view/TWiki/WikiSyntax',\n",
       "  'HTTP/1.1\"',\n",
       "  '200',\n",
       "  '7352'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:20:55',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/view/Main/DCCAndPostFix',\n",
       "  'HTTP/1.1\"',\n",
       "  '200',\n",
       "  '5253'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:23:12',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/oops/TWiki/AppendixFileSystem?template=oopsmore&param1=1.12&param2=1.12',\n",
       "  'HTTP/1.1\"',\n",
       "  '200',\n",
       "  '11382'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:24:16',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/view/Main/PeterThoeny',\n",
       "  'HTTP/1.1\"',\n",
       "  '200',\n",
       "  '4924'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:29:16',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/edit/Main/Header_checks?topicparent=Main.ConfigurationVariables',\n",
       "  'HTTP/1.1\"',\n",
       "  '401',\n",
       "  '12851'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:30:29',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/attach/Main/OfficeLocations',\n",
       "  'HTTP/1.1\"',\n",
       "  '401',\n",
       "  '12851'],\n",
       " ['64.242.88.10',\n",
       "  '-',\n",
       "  '-',\n",
       "  '[07/Mar/2004:16:31:48',\n",
       "  '-0800]',\n",
       "  '\"GET',\n",
       "  '/twiki/bin/view/TWiki/WebTopicEditTemplate',\n",
       "  'HTTP/1.1\"',\n",
       "  '200',\n",
       "  '3732']]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = sc.textFile(\"apache.access.log.txt\")\n",
    "\n",
    "file.map(lambda x: x.split()).countByKey().take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
