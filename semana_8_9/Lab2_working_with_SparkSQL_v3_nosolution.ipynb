{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 09:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"RDDBasics\")\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 : Working with Spark SQL\n",
    "\n",
    "#### We will review :\n",
    "\n",
    "1. Loading CSV file formats using SparkSession\n",
    "2. Creating DataFrame without inferring Schema \n",
    "3. Creating DataFrame inferring Schema \n",
    "4. Doing some preliminary analysis using Spark SQL on this dataset\n",
    "5. Creating UDFs (User Defined Functions) and using them on the dataset\n",
    "5. Saving a DataFrame into partitioned parquet files format\n",
    "\n",
    "#### Small (Lab) Dataset :\n",
    "\n",
    "* Air flight data - subset of ~ 100 MB (for demonstration purposes)\n",
    "* Available in the IE cluster @: /data/shared/spark/flight_data/csv_tiny\n",
    "\n",
    "#### Larger Dataset (Further Labs) :\n",
    "\n",
    "* Air flight data - subset of ~ 2.5 GB (for cluster operation purposes)\n",
    "* Available in the IE cluster @: /data/shared/spark/flight_data/csv_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Let's start by :\n",
    "# 1. Definining SPARK_HOME variable \n",
    "# 2. Using findspark to  let us work with Spark installation in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javiortig/.local/lib/python3.10/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all available data files into a data frame\n",
    "df = spark.read \\\n",
    "    .csv(\"flights.csv\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check the data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok , but the column names are not very telling. \n",
    "* How to improve this? , by telling Spark to use the header ( if exists )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"flights.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: string (nullable = true)\n",
      " |-- MONTH: string (nullable = true)\n",
      " |-- DAY: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: string (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: string (nullable = true)\n",
      " |-- DEPARTURE_DELAY: string (nullable = true)\n",
      " |-- TAXI_OUT: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- SCHEDULED_TIME: string (nullable = true)\n",
      " |-- ELAPSED_TIME: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      " |-- TAXI_IN: string (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: string (nullable = true)\n",
      " |-- ARRIVAL_TIME: string (nullable = true)\n",
      " |-- ARRIVAL_DELAY: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- AIRLINE_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Better , but still one caveat though , all values are interpreted as string, while some of them (actually most), are of numeric nature ( e.g ) Year , Month , Flight Number\n",
    "* How to improve this ?, by either telling Spark what schema to use OR telling it to infer the Schema of the data\n",
    "* Note : Asking Spark to infer schema may have a performance impact depending on the number of rows required to infer the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"flights.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'MONTH',\n",
       " 'DAY',\n",
       " 'DAY_OF_WEEK',\n",
       " 'AIRLINE',\n",
       " 'FLIGHT_NUMBER',\n",
       " 'TAIL_NUMBER',\n",
       " 'ORIGIN_AIRPORT',\n",
       " 'DESTINATION_AIRPORT',\n",
       " 'SCHEDULED_DEPARTURE',\n",
       " 'DEPARTURE_TIME',\n",
       " 'DEPARTURE_DELAY',\n",
       " 'TAXI_OUT',\n",
       " 'WHEELS_OFF',\n",
       " 'SCHEDULED_TIME',\n",
       " 'ELAPSED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'DISTANCE',\n",
       " 'WHEELS_ON',\n",
       " 'TAXI_IN',\n",
       " 'SCHEDULED_ARRIVAL',\n",
       " 'ARRIVAL_TIME',\n",
       " 'ARRIVAL_DELAY',\n",
       " 'DIVERTED',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_REASON',\n",
       " 'AIR_SYSTEM_DELAY',\n",
       " 'SECURITY_DELAY',\n",
       " 'AIRLINE_DELAY',\n",
       " 'LATE_AIRCRAFT_DELAY',\n",
       " 'WEATHER_DELAY']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 09:49:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiortig/.conda/envs/procesamiento_datos/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Register a table named flights for later SQL queries\n",
    "df.registerTempTable(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worth Noting\n",
    "\n",
    "* registerTempTable() creates an in-memory table avaialble within cluster in which it was created. The data is stored using Hive's in-memory columnar format and will only 'live' for the duration of the session.\n",
    "\n",
    "* saveAsTable() creates a permanent, physical table stored using the Parquet format. This table is accessible to all clusters including external clusters and in between sessions. The table metadata including the location of the file(s) is stored within the Hive metastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the following columns from the full dataset\n",
    "\n",
    "     YEAR,\n",
    "     MONTH,\n",
    "     DAY,\n",
    "     DAY_OF_WEEK,\n",
    "     AIRLINE,\n",
    "     FLIGHT_NUMBER,\n",
    "     TAIL_NUMBER,\n",
    "     ORIGIN_AIRPORT,\n",
    "     DESTINATION_AIRPORT,\n",
    "     SCHEDULED_DEPARTURE,\n",
    "     DEPARTURE_TIME,\n",
    "     DEPARTURE_DELAY,\n",
    "     TAXI_OUT,\n",
    "     WHEELS_OFF,\n",
    "     SCHEDULED_TIME,\n",
    "     ELAPSED_TIME,\n",
    "     AIR_TIME,\n",
    "     DISTANCE,\n",
    "     CANCELLED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'MONTH',\n",
       " 'DAY',\n",
       " 'DAY_OF_WEEK',\n",
       " 'AIRLINE',\n",
       " 'FLIGHT_NUMBER',\n",
       " 'TAIL_NUMBER',\n",
       " 'ORIGIN_AIRPORT',\n",
       " 'DESTINATION_AIRPORT',\n",
       " 'SCHEDULED_DEPARTURE',\n",
       " 'DEPARTURE_TIME',\n",
       " 'DEPARTURE_DELAY',\n",
       " 'TAXI_OUT',\n",
       " 'WHEELS_OFF',\n",
       " 'SCHEDULED_TIME',\n",
       " 'ELAPSED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'DISTANCE',\n",
       " 'WHEELS_ON',\n",
       " 'TAXI_IN',\n",
       " 'SCHEDULED_ARRIVAL',\n",
       " 'ARRIVAL_TIME',\n",
       " 'ARRIVAL_DELAY',\n",
       " 'DIVERTED',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_REASON',\n",
       " 'AIR_SYSTEM_DELAY',\n",
       " 'SECURITY_DELAY',\n",
       " 'AIRLINE_DELAY',\n",
       " 'LATE_AIRCRAFT_DELAY',\n",
       " 'WEATHER_DELAY']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset=spark.sql(\"\"\"\n",
    "select\n",
    " YEAR,\n",
    " MONTH,\n",
    " DAY,\n",
    " DAY_OF_WEEK,\n",
    " AIRLINE,\n",
    " FLIGHT_NUMBER,\n",
    " TAIL_NUMBER,\n",
    " ORIGIN_AIRPORT,\n",
    " DESTINATION_AIRPORT,\n",
    " SCHEDULED_DEPARTURE,\n",
    " DEPARTURE_TIME,\n",
    " DEPARTURE_DELAY,\n",
    " TAXI_OUT,\n",
    " WHEELS_OFF,\n",
    " SCHEDULED_TIME,\n",
    " ELAPSED_TIME,\n",
    " AIR_TIME,\n",
    " DISTANCE,\n",
    " CANCELLED\n",
    "FROM\n",
    " flights\n",
    "\"\"\"\n",
    ")\n",
    "# OR \n",
    "# selection=[\"year,month,dayofmonth,dayofweek,\"\n",
    "#       \"flightnum,origin,dest,deptime,depdelay,\"\n",
    "#       \"arrtime,arrdelay,cancelled,cancellationcode,\"\n",
    "#       \"airtime,distance \"]\n",
    "# info.select(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='AS', FLIGHT_NUMBER=98, TAIL_NUMBER='N407AS', ORIGIN_AIRPORT='ANC', DESTINATION_AIRPORT='SEA', SCHEDULED_DEPARTURE=5, DEPARTURE_TIME=2354, DEPARTURE_DELAY=-11, TAXI_OUT=21, WHEELS_OFF=15, SCHEDULED_TIME=205, ELAPSED_TIME=194, AIR_TIME=169, DISTANCE=1448, CANCELLED=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache this DataFrame \n",
    "df_subset.cache()\n",
    "# Cache the flights table\n",
    "spark.sql(\"cache table flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='AS', FLIGHT_NUMBER=98, TAIL_NUMBER='N407AS', ORIGIN_AIRPORT='ANC', DESTINATION_AIRPORT='SEA', SCHEDULED_DEPARTURE=5, DEPARTURE_TIME=2354, DEPARTURE_DELAY=-11, TAXI_OUT=21, WHEELS_OFF=15, SCHEDULED_TIME=205, ELAPSED_TIME=194, AIR_TIME=169, DISTANCE=1448, CANCELLED=0),\n",
       " Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='AA', FLIGHT_NUMBER=2336, TAIL_NUMBER='N3KUAA', ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='PBI', SCHEDULED_DEPARTURE=10, DEPARTURE_TIME=2, DEPARTURE_DELAY=-8, TAXI_OUT=12, WHEELS_OFF=14, SCHEDULED_TIME=280, ELAPSED_TIME=279, AIR_TIME=263, DISTANCE=2330, CANCELLED=0),\n",
       " Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='US', FLIGHT_NUMBER=840, TAIL_NUMBER='N171US', ORIGIN_AIRPORT='SFO', DESTINATION_AIRPORT='CLT', SCHEDULED_DEPARTURE=20, DEPARTURE_TIME=18, DEPARTURE_DELAY=-2, TAXI_OUT=16, WHEELS_OFF=34, SCHEDULED_TIME=286, ELAPSED_TIME=293, AIR_TIME=266, DISTANCE=2296, CANCELLED=0),\n",
       " Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='AA', FLIGHT_NUMBER=258, TAIL_NUMBER='N3HYAA', ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='MIA', SCHEDULED_DEPARTURE=20, DEPARTURE_TIME=15, DEPARTURE_DELAY=-5, TAXI_OUT=15, WHEELS_OFF=30, SCHEDULED_TIME=285, ELAPSED_TIME=281, AIR_TIME=258, DISTANCE=2342, CANCELLED=0),\n",
       " Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='AS', FLIGHT_NUMBER=135, TAIL_NUMBER='N527AS', ORIGIN_AIRPORT='SEA', DESTINATION_AIRPORT='ANC', SCHEDULED_DEPARTURE=25, DEPARTURE_TIME=24, DEPARTURE_DELAY=-1, TAXI_OUT=11, WHEELS_OFF=35, SCHEDULED_TIME=235, ELAPSED_TIME=215, AIR_TIME=199, DISTANCE=1448, CANCELLED=0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 5 rows of the subset data to get a feeling of what to expect\n",
    "df_subset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='AS', FLIGHT_NUMBER=98, TAIL_NUMBER='N407AS', ORIGIN_AIRPORT='ANC', DESTINATION_AIRPORT='SEA', SCHEDULED_DEPARTURE=5, DEPARTURE_TIME=2354, DEPARTURE_DELAY=-11, TAXI_OUT=21, WHEELS_OFF=15, SCHEDULED_TIME=205, ELAPSED_TIME=194, AIR_TIME=169, DISTANCE=1448, CANCELLED=0),\n",
       " Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='AA', FLIGHT_NUMBER=2336, TAIL_NUMBER='N3KUAA', ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='PBI', SCHEDULED_DEPARTURE=10, DEPARTURE_TIME=2, DEPARTURE_DELAY=-8, TAXI_OUT=12, WHEELS_OFF=14, SCHEDULED_TIME=280, ELAPSED_TIME=279, AIR_TIME=263, DISTANCE=2330, CANCELLED=0),\n",
       " Row(YEAR=2015, MONTH=1, DAY=1, DAY_OF_WEEK=4, AIRLINE='US', FLIGHT_NUMBER=840, TAIL_NUMBER='N171US', ORIGIN_AIRPORT='SFO', DESTINATION_AIRPORT='CLT', SCHEDULED_DEPARTURE=20, DEPARTURE_TIME=18, DEPARTURE_DELAY=-2, TAXI_OUT=16, WHEELS_OFF=34, SCHEDULED_TIME=286, ELAPSED_TIME=293, AIR_TIME=266, DISTANCE=2296, CANCELLED=0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some SQL queries ( use both the DataFrame and direct SQL queries )\n",
    "\n",
    "1. Find the number of departing flights from a given airport\n",
    "2. Find the total number of delayed flights on a given airport\n",
    "3. Find the average delay per airport\n",
    "4. Find the top 5 airports with the highest average delays\n",
    "5. Find the worst airport in terms of total nb cancelled flights (cancelled=1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.9 s\n",
      "Total nb.of flights: 5819079\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 5819079|\n",
      "+--------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "# how many records do we have in total?\n",
    "%time total=df_subset.count()\n",
    "print('Total nb.of flights: %d' % total)\n",
    "# OR in SQL\n",
    "%time spark.sql(\"select COUNT(*) from flights\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departing from JFK : 93811 \n",
      "Delayed   from JFK : 19231 \n",
      "Delayed Percentage : 20.499728 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1.2. how many flights and delayed\n",
    "def statsByAirport(airport_id,df):\n",
    "    from_id=df.filter(df['ORIGIN_AIRPORT']==airport_id)\n",
    "    delayed=from_id.filter(df['DEPARTURE_DELAY']>=15.0)\n",
    "    ndep=from_id.count()\n",
    "    ndel=delayed.count()\n",
    "    return (ndep,ndel)\n",
    "    \n",
    "airport='JFK'\n",
    "\n",
    "n,m=statsByAirport(airport,df_subset)\n",
    "\n",
    "print('Departing from %s : %d ' %(airport,n))\n",
    "print('Delayed   from %s : %d ' %(airport,m))\n",
    "print('Delayed Percentage : %f %%' %((m/n)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport : JFK \n",
      "Average delay : 12.128115 min\n"
     ]
    }
   ],
   "source": [
    "# 3. Average delay per flight on an airport\n",
    "def averageDelay(airport_id,df):\n",
    "    from_id=df.filter(df['ORIGIN_AIRPORT']==airport_id)\n",
    "    return from_id.select('DEPARTURE_DELAY').describe() # returns a dataframe with descriptive statistics\n",
    "\n",
    "airport='JFK'\n",
    "df=averageDelay(airport,df_subset)\n",
    "\n",
    "print('Airport : %s ' %(airport))\n",
    "print('Average delay : %f min' %(float(df.collect()[1]['DEPARTURE_DELAY'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|origin|          depdelay|\n",
      "+------+------------------+\n",
      "| 14222| 89.11111111111111|\n",
      "|   ILG|29.391752577319586|\n",
      "| 13964|          28.84375|\n",
      "|   MVY| 25.90731707317073|\n",
      "|   HYA|23.182926829268293|\n",
      "| 10154|22.857142857142858|\n",
      "| 10581| 20.11111111111111|\n",
      "|   STC|18.692307692307693|\n",
      "|   OTH|17.777358490566037|\n",
      "|   ASE| 17.58753799392097|\n",
      "| 10165|17.555555555555557|\n",
      "| 14025| 17.53846153846154|\n",
      "|   CEC|17.413793103448278|\n",
      "|   GST| 17.17105263157895|\n",
      "|   BPT| 17.02085620197585|\n",
      "|   GUM|16.647590361445783|\n",
      "|   ACK| 16.38888888888889|\n",
      "|   UST|16.368055555555557|\n",
      "|   EGE|15.744398340248962|\n",
      "|   PPG|15.102803738317757|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Top 5 airports with highest average delay : actually easier here with SQL AVG\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    ORIGIN_AIRPORT as origin,\n",
    "    AVG(DEPARTURE_DELAY) as depdelay \n",
    "FROM \n",
    "    flights \n",
    "GROUP BY \n",
    "    origin \n",
    "ORDER BY \n",
    "    avg(DEPARTURE_DELAY) DESC\n",
    "\"\"\"\n",
    "df_delays=spark.sql(query)\n",
    "df_delays.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "CANCELATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|origin|num_cancelations|\n",
      "+------+----------------+\n",
      "|   ORD|            8548|\n",
      "|   DFW|            6254|\n",
      "|   LGA|            4531|\n",
      "|   EWR|            3110|\n",
      "|   BOS|            2654|\n",
      "|   ATL|            2557|\n",
      "|   LAX|            2164|\n",
      "|   SFO|            2148|\n",
      "|   IAH|            2130|\n",
      "|   DEN|            2123|\n",
      "+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CANCELATIONS\n",
    "# 4. Top 10 airports with highest number of cancelations\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    ORIGIN_AIRPORT as origin,\n",
    "    SUM(CANCELLED) as num_cancelations\n",
    "FROM \n",
    "    flights \n",
    "GROUP BY \n",
    "    origin \n",
    "ORDER BY \n",
    "    num_cancelations DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(s)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UDF - Register a function as a UDF\n",
    "# https://docs.databricks.com/spark/latest/spark-sql/udf-python.html\n",
    "\n",
    "def squared(s):\n",
    "  return s * s\n",
    "spark.udf.register(\"squaredWithPython\", squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. UDF\n",
    "def cancellation_reverse(airport_id):\n",
    "    return airport_id[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"cancellation_reverse\", lambda x : cancellation_reverse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|origin_reverse|num_cancelations|\n",
      "+--------------+----------------+\n",
      "|           DRO|            8548|\n",
      "|           WFD|            6254|\n",
      "|           AGL|            4531|\n",
      "|           RWE|            3110|\n",
      "|           SOB|            2654|\n",
      "|           LTA|            2557|\n",
      "|           XAL|            2164|\n",
      "|           OFS|            2148|\n",
      "|           HAI|            2130|\n",
      "|           NED|            2123|\n",
      "|           ACD|            2027|\n",
      "|           WDM|            1959|\n",
      "|           KFJ|            1922|\n",
      "|           IWB|            1533|\n",
      "|           WTD|            1270|\n",
      "|           OCM|            1118|\n",
      "|           TLC|            1081|\n",
      "|           LHP|            1074|\n",
      "|           ANB|            1066|\n",
      "|           SAL|             936|\n",
      "+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the function with Spark SQL as User Defined Function\n",
    "# spark.udf.register(\"cancellation_reverse\", lambda x : cancellation_reverse(x))\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    cancellation_reverse(ORIGIN_AIRPORT) AS origin_reverse,\n",
    "    SUM(CANCELLED) as num_cancelations\n",
    "FROM \n",
    "    flights \n",
    "GROUP BY \n",
    "    origin_reverse \n",
    "ORDER BY num_cancelations DESC\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save this dataframe in parquet (columnar) format for boost in loading performance\n",
    "* In order to do we want to 'be clever' and partition the data by specific atributes , in this case\n",
    "* Year and Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting...\n",
      "Writting Completed!\n"
     ]
    }
   ],
   "source": [
    "# Save the data into my HOME\n",
    "# IMPORTANT NOTE: we are partinioning (structuring)\n",
    "# by relevant factors in our data , in this case year and month\n",
    "# can be used to naturally save this data.\n",
    "print('Writting...')\n",
    "my_home=os.environ['HOME']\n",
    "out_dir=\"airline_data\"\n",
    "df_subset.write.partitionBy(\n",
    "        \"YEAR\",\"MONTH\"\n",
    "    ).parquet(\n",
    "        \"file://\"\n",
    "        + my_home\n",
    "        +'/'\n",
    "        + out_dir,\n",
    "        mode='overwrite'\n",
    "    )\n",
    "print('Writting Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mgadi'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HOME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISES: \n",
    "1. Using %time, show which one is faster df_subset.count() or spark.sql(\"select COUNT(*) from flights\").show()?\n",
    "1. Report the top 10 airport with most departures in the dataset. Make both use of the DataFrame API and a direct SQL query for this.\n",
    "1. What is the flight with the longest delay?\n",
    "1. Report the best , top 5 , carriers ( column carrier ) in terms of smallest average departure delay on all airports. Consider a flight delayed that one where depdelay > 0 min\n",
    "1. Which destinations are most likely to get delays from JFK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================>                   (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|ORIGIN_AIRPORT| count|\n",
      "+--------------+------+\n",
      "|           ATL|346836|\n",
      "|           ORD|285884|\n",
      "|           DFW|239551|\n",
      "|           DEN|196055|\n",
      "|           LAX|194673|\n",
      "|           SFO|148008|\n",
      "|           PHX|146815|\n",
      "|           IAH|146622|\n",
      "|           LAS|133181|\n",
      "|           MSP|112117|\n",
      "+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "df_subset.groupBy('ORIGIN_AIRPORT').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:=========>                                              (2 + 10) / 12]\r",
      "\r",
      "[Stage 11:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|ORIGIN_AIRPORT|num_flights|\n",
      "+--------------+-----------+\n",
      "|           ATL|     346836|\n",
      "|           ORD|     285884|\n",
      "|           DFW|     239551|\n",
      "|           DEN|     196055|\n",
      "|           LAX|     194673|\n",
      "|           SFO|     148008|\n",
      "|           PHX|     146815|\n",
      "|           IAH|     146622|\n",
      "|           LAS|     133181|\n",
      "|           MSP|     112117|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2 CON SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    ORIGIN_AIRPORT,\n",
    "    COUNT(*) AS num_flights\n",
    "FROM\n",
    "    FLIGHTS\n",
    "GROUP BY\n",
    "    ORIGIN_AIRPORT\n",
    "ORDER BY\n",
    "    num_flights DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "df_subset.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|AIRLINE|avg(DEPARTURE_DELAY)|\n",
      "+-------+--------------------+\n",
      "|     HA|  16.844038518812667|\n",
      "|     AS|  26.045976219988063|\n",
      "|     WN|   26.95237708779179|\n",
      "|     US|  28.500615360025574|\n",
      "|     DL|   29.68744224907333|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "df_subset.filter(df_subset['DEPARTURE_DELAY'] > 0) \\\n",
    "    .select('AIRLINE', 'DEPARTURE_DELAY') \\\n",
    "    .groupBy('AIRLINE') \\\n",
    "    .mean('DEPARTURE_DELAY') \\\n",
    "    .orderBy('avg(DEPARTURE_DELAY)', ascending=True) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "# how many records do we have in total?\n",
    "%time total=df_subset.count()\n",
    "print('Total nb.of flights: %d' % total)\n",
    "# OR in SQL\n",
    "%time spark.sql(\"select COUNT(*) from flights\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'sum(withDelay)' due to data type mismatch: function sum requires numeric or interval types, not boolean;\n'Aggregate [DESTINATION_AIRPORT#183], [DESTINATION_AIRPORT#183, sum(withDelay#8620) AS sum(withDelay)#8661]\n+- Project [YEAR#175, MONTH#176, DAY#177, DAY_OF_WEEK#178, AIRLINE#179, FLIGHT_NUMBER#180, TAIL_NUMBER#181, ORIGIN_AIRPORT#182, DESTINATION_AIRPORT#183, SCHEDULED_DEPARTURE#184, DEPARTURE_TIME#185, DEPARTURE_DELAY#186, TAXI_OUT#187, WHEELS_OFF#188, SCHEDULED_TIME#189, ELAPSED_TIME#190, AIR_TIME#191, DISTANCE#192, CANCELLED#199, (DEPARTURE_DELAY#186 > 0) AS withDelay#8620]\n   +- Filter (ORIGIN_AIRPORT#182 = JFK)\n      +- Project [YEAR#175, MONTH#176, DAY#177, DAY_OF_WEEK#178, AIRLINE#179, FLIGHT_NUMBER#180, TAIL_NUMBER#181, ORIGIN_AIRPORT#182, DESTINATION_AIRPORT#183, SCHEDULED_DEPARTURE#184, DEPARTURE_TIME#185, DEPARTURE_DELAY#186, TAXI_OUT#187, WHEELS_OFF#188, SCHEDULED_TIME#189, ELAPSED_TIME#190, AIR_TIME#191, DISTANCE#192, CANCELLED#199]\n         +- SubqueryAlias flights\n            +- View (`flights`, [YEAR#175,MONTH#176,DAY#177,DAY_OF_WEEK#178,AIRLINE#179,FLIGHT_NUMBER#180,TAIL_NUMBER#181,ORIGIN_AIRPORT#182,DESTINATION_AIRPORT#183,SCHEDULED_DEPARTURE#184,DEPARTURE_TIME#185,DEPARTURE_DELAY#186,TAXI_OUT#187,WHEELS_OFF#188,SCHEDULED_TIME#189,ELAPSED_TIME#190,AIR_TIME#191,DISTANCE#192,WHEELS_ON#193,TAXI_IN#194,SCHEDULED_ARRIVAL#195,ARRIVAL_TIME#196,ARRIVAL_DELAY#197,DIVERTED#198,CANCELLED#199,CANCELLATION_REASON#200,AIR_SYSTEM_DELAY#201,SECURITY_DELAY#202,AIRLINE_DELAY#203,LATE_AIRCRAFT_DELAY#204,WEATHER_DELAY#205])\n               +- Relation [YEAR#175,MONTH#176,DAY#177,DAY_OF_WEEK#178,AIRLINE#179,FLIGHT_NUMBER#180,TAIL_NUMBER#181,ORIGIN_AIRPORT#182,DESTINATION_AIRPORT#183,SCHEDULED_DEPARTURE#184,DEPARTURE_TIME#185,DEPARTURE_DELAY#186,TAXI_OUT#187,WHEELS_OFF#188,SCHEDULED_TIME#189,ELAPSED_TIME#190,AIR_TIME#191,DISTANCE#192,WHEELS_ON#193,TAXI_IN#194,SCHEDULED_ARRIVAL#195,ARRIVAL_TIME#196,ARRIVAL_DELAY#197,DIVERTED#198,... 7 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5605/2762178531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Which destinations are most likely to get delays from JFK?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_subset\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ORIGIN_AIRPORT'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'JFK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"withDelay\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEPARTURE_DELAY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/procesamiento_datos/lib/python3.9/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exprs should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/procesamiento_datos/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'sum(withDelay)' due to data type mismatch: function sum requires numeric or interval types, not boolean;\n'Aggregate [DESTINATION_AIRPORT#183], [DESTINATION_AIRPORT#183, sum(withDelay#8620) AS sum(withDelay)#8661]\n+- Project [YEAR#175, MONTH#176, DAY#177, DAY_OF_WEEK#178, AIRLINE#179, FLIGHT_NUMBER#180, TAIL_NUMBER#181, ORIGIN_AIRPORT#182, DESTINATION_AIRPORT#183, SCHEDULED_DEPARTURE#184, DEPARTURE_TIME#185, DEPARTURE_DELAY#186, TAXI_OUT#187, WHEELS_OFF#188, SCHEDULED_TIME#189, ELAPSED_TIME#190, AIR_TIME#191, DISTANCE#192, CANCELLED#199, (DEPARTURE_DELAY#186 > 0) AS withDelay#8620]\n   +- Filter (ORIGIN_AIRPORT#182 = JFK)\n      +- Project [YEAR#175, MONTH#176, DAY#177, DAY_OF_WEEK#178, AIRLINE#179, FLIGHT_NUMBER#180, TAIL_NUMBER#181, ORIGIN_AIRPORT#182, DESTINATION_AIRPORT#183, SCHEDULED_DEPARTURE#184, DEPARTURE_TIME#185, DEPARTURE_DELAY#186, TAXI_OUT#187, WHEELS_OFF#188, SCHEDULED_TIME#189, ELAPSED_TIME#190, AIR_TIME#191, DISTANCE#192, CANCELLED#199]\n         +- SubqueryAlias flights\n            +- View (`flights`, [YEAR#175,MONTH#176,DAY#177,DAY_OF_WEEK#178,AIRLINE#179,FLIGHT_NUMBER#180,TAIL_NUMBER#181,ORIGIN_AIRPORT#182,DESTINATION_AIRPORT#183,SCHEDULED_DEPARTURE#184,DEPARTURE_TIME#185,DEPARTURE_DELAY#186,TAXI_OUT#187,WHEELS_OFF#188,SCHEDULED_TIME#189,ELAPSED_TIME#190,AIR_TIME#191,DISTANCE#192,WHEELS_ON#193,TAXI_IN#194,SCHEDULED_ARRIVAL#195,ARRIVAL_TIME#196,ARRIVAL_DELAY#197,DIVERTED#198,CANCELLED#199,CANCELLATION_REASON#200,AIR_SYSTEM_DELAY#201,SECURITY_DELAY#202,AIRLINE_DELAY#203,LATE_AIRCRAFT_DELAY#204,WEATHER_DELAY#205])\n               +- Relation [YEAR#175,MONTH#176,DAY#177,DAY_OF_WEEK#178,AIRLINE#179,FLIGHT_NUMBER#180,TAIL_NUMBER#181,ORIGIN_AIRPORT#182,DESTINATION_AIRPORT#183,SCHEDULED_DEPARTURE#184,DEPARTURE_TIME#185,DEPARTURE_DELAY#186,TAXI_OUT#187,WHEELS_OFF#188,SCHEDULED_TIME#189,ELAPSED_TIME#190,AIR_TIME#191,DISTANCE#192,WHEELS_ON#193,TAXI_IN#194,SCHEDULED_ARRIVAL#195,ARRIVAL_TIME#196,ARRIVAL_DELAY#197,DIVERTED#198,... 7 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "# Which destinations are most likely to get delays from JFK?\n",
    "df_subset \\\n",
    "    .filter(df_subset['ORIGIN_AIRPORT'] == 'JFK') \\\n",
    "    .withColumn(\"withDelay\", df_subset['DEPARTURE_DELAY']>0) \\\n",
    "    .groupBy('DESTINATION_AIRPORT') \\\n",
    "    .agg({'withDelay': 'sum'}) \\\n",
    "    .show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
